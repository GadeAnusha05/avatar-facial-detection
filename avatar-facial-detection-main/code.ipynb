{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2a6565-f008-413f-887d-861910d5bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Mediapipe setup\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=5,  # Support up to 5 faces\n",
    "    refine_landmarks=True\n",
    ")\n",
    "drawing_utils = mp.solutions.drawing_utils\n",
    "LIPS = mp_face_mesh.FACEMESH_LIPS\n",
    "LEFT_IRIS = [468, 469, 470, 471]\n",
    "RIGHT_IRIS = [473, 474, 475, 476]\n",
    "\n",
    "# Emotion rules\n",
    "emotions = {\n",
    "    \"happy\": {\"emoji\": \"😊\", \"color\": (0, 255, 0)},\n",
    "    \"sad\": {\"emoji\": \"😢\", \"color\": (255, 0, 0)},\n",
    "    \"angry\": {\"emoji\": \"😠\", \"color\": (0, 0, 255)},\n",
    "    \"surprise\": {\"emoji\": \"😲\", \"color\": (0, 255, 255)},\n",
    "    \"neutral\": {\"emoji\": \"😐\", \"color\": (255, 255, 255)},\n",
    "    \"fear\": {\"emoji\": \"😨\", \"color\": (255, 140, 0)},\n",
    "    \"disgust\": {\"emoji\": \"🤢\", \"color\": (138, 43, 226)}\n",
    "}\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return np.linalg.norm(np.array([p1.x, p1.y]) - np.array([p2.x, p2.y]))\n",
    "\n",
    "def get_emotion(landmarks):\n",
    "    top_lip = landmarks[13]\n",
    "    bottom_lip = landmarks[14]\n",
    "    left_mouth = landmarks[61]\n",
    "    right_mouth = landmarks[291]\n",
    "    left_eye_top = landmarks[159]\n",
    "    left_eye_bottom = landmarks[145]\n",
    "    right_eye_top = landmarks[386]\n",
    "    right_eye_bottom = landmarks[374]\n",
    "    iris_left = landmarks[468]\n",
    "\n",
    "    face_width = distance(landmarks[234], landmarks[454])\n",
    "    mouth_open = distance(top_lip, bottom_lip) / face_width\n",
    "    mouth_stretch = distance(left_mouth, right_mouth) / face_width\n",
    "    eye_open = (distance(left_eye_top, left_eye_bottom) + distance(right_eye_top, right_eye_bottom)) / (2 * face_width)\n",
    "    eye_center_y = (left_eye_top.y + left_eye_bottom.y + right_eye_top.y + right_eye_bottom.y) / 4\n",
    "    sad_offset = iris_left.y - eye_center_y\n",
    "\n",
    "    if mouth_stretch > 0.40 and mouth_open < 0.06:\n",
    "        return \"happy\"\n",
    "    elif mouth_open >= 0.12:\n",
    "        return \"surprise\"\n",
    "    elif 0.06 < mouth_open < 0.12:\n",
    "        return \"fear\"\n",
    "    elif sad_offset > 0.01 and eye_open < 0.04:\n",
    "        return \"sad\"\n",
    "    elif mouth_open < 0.03 and eye_open < 0.08 and mouth_stretch < 0.38:\n",
    "        return \"disgust\"\n",
    "    elif eye_open > 0.096 and mouth_open < 0.06:\n",
    "        return \"angry\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "    avatar_canvas = np.zeros_like(frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            landmark_list = face_landmarks.landmark\n",
    "            emotion = get_emotion(landmark_list)\n",
    "            color = emotions[emotion][\"color\"]\n",
    "            emoji = emotions[emotion][\"emoji\"]\n",
    "\n",
    "            # Draw mesh\n",
    "            drawing_utils.draw_landmarks(\n",
    "                avatar_canvas, face_landmarks,\n",
    "                mp_face_mesh.FACEMESH_TESSELATION, None,\n",
    "                drawing_utils.DrawingSpec(color=color, thickness=1, circle_radius=1)\n",
    "            )\n",
    "            drawing_utils.draw_landmarks(\n",
    "                avatar_canvas, face_landmarks, LIPS, None,\n",
    "                drawing_utils.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1)\n",
    "            )\n",
    "\n",
    "            # Draw irises\n",
    "            for idx in LEFT_IRIS + RIGHT_IRIS:\n",
    "                pt = landmark_list[idx]\n",
    "                cx, cy = int(pt.x * w), int(pt.y * h)\n",
    "                cv2.circle(avatar_canvas, (cx, cy), 2, (0, 255, 255), -1)\n",
    "\n",
    "            # Show emotion\n",
    "            cx, cy = int(landmark_list[0].x * w), int(landmark_list[0].y * h)\n",
    "            cv2.putText(avatar_canvas, f\"{emotion.upper()} {emoji}\", (cx - 50, cy - 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "    # Show windows\n",
    "    cv2.imshow(\"Webcam Feed\", cv2.resize(frame, (640, 480)))\n",
    "    cv2.imshow(\"Avatar Emotion Mesh\", cv2.resize(avatar_canvas, (640, 480)))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3049a145-5aa0-4512-bbeb-3550b8f278e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Mediapipe setup\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=5,\n",
    "    refine_landmarks=True\n",
    ")\n",
    "drawing_utils = mp.solutions.drawing_utils\n",
    "LIPS = mp_face_mesh.FACEMESH_LIPS\n",
    "LEFT_IRIS = [468, 469, 470, 471]\n",
    "RIGHT_IRIS = [473, 474, 475, 476]\n",
    "\n",
    "# Emotion configuration\n",
    "emotions = {\n",
    "    \"happy\": {\"emoji\": \"😊\", \"color\": (0, 255, 0)},\n",
    "    \"sad\": {\"emoji\": \"😢\", \"color\": (255, 0, 0)},\n",
    "    \"angry\": {\"emoji\": \"😠\", \"color\": (0, 0, 255)},\n",
    "    \"surprise\": {\"emoji\": \"😲\", \"color\": (0, 255, 255)},\n",
    "    \"neutral\": {\"emoji\": \"😐\", \"color\": (255, 255, 255)},\n",
    "    \"fear\": {\"emoji\": \"😨\", \"color\": (255, 140, 0)},\n",
    "    \"disgust\": {\"emoji\": \"🤢\", \"color\": (138, 43, 226)}\n",
    "}\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return np.linalg.norm(np.array([p1.x, p1.y]) - np.array([p2.x, p2.y]))\n",
    "\n",
    "def get_emotion(landmarks):\n",
    "    top_lip = landmarks[13]\n",
    "    bottom_lip = landmarks[14]\n",
    "    left_mouth = landmarks[61]\n",
    "    right_mouth = landmarks[291]\n",
    "    left_eye_top = landmarks[159]\n",
    "    left_eye_bottom = landmarks[145]\n",
    "    right_eye_top = landmarks[386]\n",
    "    right_eye_bottom = landmarks[374]\n",
    "    iris_left = landmarks[468]\n",
    "\n",
    "    face_width = distance(landmarks[234], landmarks[454])\n",
    "    mouth_open = distance(top_lip, bottom_lip) / face_width\n",
    "    mouth_stretch = distance(left_mouth, right_mouth) / face_width\n",
    "    eye_open = (distance(left_eye_top, left_eye_bottom) + distance(right_eye_top, right_eye_bottom)) / (2 * face_width)\n",
    "    eye_center_y = (left_eye_top.y + left_eye_bottom.y + right_eye_top.y + right_eye_bottom.y) / 4\n",
    "    sad_offset = iris_left.y - eye_center_y\n",
    "\n",
    "    if mouth_stretch > 0.40 and mouth_open < 0.06:\n",
    "        return \"happy\"\n",
    "    elif mouth_open >= 0.12:\n",
    "        return \"surprise\"\n",
    "    elif 0.06 < mouth_open < 0.12:\n",
    "        return \"fear\"\n",
    "    elif sad_offset > 0.01 and eye_open < 0.04:\n",
    "        return \"sad\"\n",
    "    elif mouth_open < 0.03 and eye_open < 0.08 and mouth_stretch < 0.38:\n",
    "        return \"disgust\"\n",
    "    elif eye_open > 0.096 and mouth_open < 0.06:\n",
    "        return \"angry\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    avatar_canvas = np.zeros_like(frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            landmark_list = face_landmarks.landmark\n",
    "            emotion = get_emotion(landmark_list)\n",
    "            color = emotions[emotion][\"color\"]\n",
    "            emoji = emotions[emotion][\"emoji\"]\n",
    "\n",
    "            # Draw avatar mesh\n",
    "            drawing_utils.draw_landmarks(\n",
    "                avatar_canvas, face_landmarks,\n",
    "                mp_face_mesh.FACEMESH_TESSELATION, None,\n",
    "                drawing_utils.DrawingSpec(color=color, thickness=1, circle_radius=1)\n",
    "            )\n",
    "            drawing_utils.draw_landmarks(\n",
    "                avatar_canvas, face_landmarks, LIPS, None,\n",
    "                drawing_utils.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1)\n",
    "            )\n",
    "\n",
    "            # Draw irises\n",
    "            for idx in LEFT_IRIS + RIGHT_IRIS:\n",
    "                pt = landmark_list[idx]\n",
    "                cx, cy = int(pt.x * w), int(pt.y * h)\n",
    "                cv2.circle(avatar_canvas, (cx, cy), 2, (0, 255, 255), -1)\n",
    "\n",
    "            # Emotion label on avatar face\n",
    "            cx, cy = int(landmark_list[1].x * w), int(landmark_list[1].y * h)\n",
    "            cv2.putText(avatar_canvas, f\"{emotion.upper()} {emoji}\", (cx - 50, cy - 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "            # Also overlay emotion text on real webcam frame\n",
    "            cv2.putText(frame, f\"{emotion.upper()} {emoji}\", (cx - 50, cy - 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "    # Resize both views to same size\n",
    "    frame_resized = cv2.resize(frame, (480, 360))\n",
    "    avatar_resized = cv2.resize(avatar_canvas, (480, 360))\n",
    "\n",
    "    # Combine side-by-side\n",
    "    combined_view = np.hstack((frame_resized, avatar_resized))\n",
    "    cv2.imshow(\"🤳 Real Face + Avatar 🤖\", combined_view)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC key to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ff5f5c-9fa2-4bd4-9a74-5213bf439d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "\n",
    "# Setup speech engine\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 160)\n",
    "\n",
    "# Setup Mediapipe\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=5, refine_landmarks=True)\n",
    "drawing_utils = mp.solutions.drawing_utils\n",
    "\n",
    "# Face mesh constants\n",
    "LIPS = mp_face_mesh.FACEMESH_LIPS\n",
    "LEFT_IRIS = [468, 469, 470, 471]\n",
    "RIGHT_IRIS = [473, 474, 475, 476]\n",
    "\n",
    "# Emotion config\n",
    "emotions = {\n",
    "    \"happy\": {\"emoji\": \"😊\", \"color\": (0, 255, 0), \"msg\": \"You look happy!\"},\n",
    "    \"sad\": {\"emoji\": \"😢\", \"color\": (255, 0, 0), \"msg\": \"You seem sad.\"},\n",
    "    \"angry\": {\"emoji\": \"😠\", \"color\": (0, 0, 255), \"msg\": \"You look angry!\"},\n",
    "    \"surprise\": {\"emoji\": \"😲\", \"color\": (0, 255, 255), \"msg\": \"You're surprised!\"},\n",
    "    \"neutral\": {\"emoji\": \"😐\", \"color\": (200, 200, 200), \"msg\": \"You look neutral.\"},\n",
    "    \"fear\": {\"emoji\": \"😨\", \"color\": (255, 140, 0), \"msg\": \"You look scared.\"},\n",
    "    \"disgust\": {\"emoji\": \"🤢\", \"color\": (138, 43, 226), \"msg\": \"You're feeling disgusted.\"}\n",
    "}\n",
    "\n",
    "prev_emotion = \"\"\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return np.linalg.norm(np.array([p1.x, p1.y]) - np.array([p2.x, p2.y]))\n",
    "\n",
    "def get_emotion(landmarks):\n",
    "    top_lip = landmarks[13]\n",
    "    bottom_lip = landmarks[14]\n",
    "    left_mouth = landmarks[61]\n",
    "    right_mouth = landmarks[291]\n",
    "    left_eye_top = landmarks[159]\n",
    "    left_eye_bottom = landmarks[145]\n",
    "    right_eye_top = landmarks[386]\n",
    "    right_eye_bottom = landmarks[374]\n",
    "    iris_left = landmarks[468]\n",
    "\n",
    "    face_width = distance(landmarks[234], landmarks[454])\n",
    "    mouth_open = distance(top_lip, bottom_lip) / face_width\n",
    "    mouth_stretch = distance(left_mouth, right_mouth) / face_width\n",
    "    eye_open = (distance(left_eye_top, left_eye_bottom) + distance(right_eye_top, right_eye_bottom)) / (2 * face_width)\n",
    "\n",
    "    eye_center_y = (left_eye_top.y + left_eye_bottom.y + right_eye_top.y + right_eye_bottom.y) / 4\n",
    "    sad_offset = iris_left.y - eye_center_y\n",
    "\n",
    "    if mouth_stretch > 0.40 and mouth_open < 0.06:\n",
    "        return \"happy\"\n",
    "    elif mouth_open >= 0.12:\n",
    "        return \"surprise\"\n",
    "    elif 0.06 < mouth_open < 0.12:\n",
    "        return \"fear\"\n",
    "    elif sad_offset > 0.01 and eye_open < 0.04:\n",
    "        return \"sad\"\n",
    "    elif mouth_open < 0.03 and eye_open < 0.08 and mouth_stretch < 0.38:\n",
    "        return \"disgust\"\n",
    "    elif eye_open > 0.096 and mouth_open < 0.06:\n",
    "        return \"angry\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    avatar_canvas = np.zeros_like(frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            landmark_list = face_landmarks.landmark\n",
    "            emotion = get_emotion(landmark_list)\n",
    "            color = emotions[emotion][\"color\"]\n",
    "            emoji = emotions[emotion][\"emoji\"]\n",
    "\n",
    "            # Draw avatar face only (not on webcam feed)\n",
    "            drawing_utils.draw_landmarks(\n",
    "                avatar_canvas, face_landmarks,\n",
    "                mp_face_mesh.FACEMESH_TESSELATION, None,\n",
    "                drawing_utils.DrawingSpec(color=color, thickness=1, circle_radius=1)\n",
    "            )\n",
    "            drawing_utils.draw_landmarks(\n",
    "                avatar_canvas, face_landmarks, LIPS, None,\n",
    "                drawing_utils.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1)\n",
    "            )\n",
    "\n",
    "            for idx in LEFT_IRIS + RIGHT_IRIS:\n",
    "                pt = landmark_list[idx]\n",
    "                cx, cy = int(pt.x * w), int(pt.y * h)\n",
    "                cv2.circle(avatar_canvas, (cx, cy), 2, (0, 255, 255), -1)\n",
    "\n",
    "            # Show emotion label in avatar\n",
    "            cx, cy = int(landmark_list[0].x * w), int(landmark_list[0].y * h)\n",
    "            cv2.putText(avatar_canvas, f\"{emotion.upper()} {emoji}\", (cx - 50, cy - 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "            # Trigger speech only when emotion changes\n",
    "            if emotion != prev_emotion:\n",
    "                engine.say(emotions[emotion][\"msg\"])\n",
    "                engine.runAndWait()\n",
    "                prev_emotion = emotion\n",
    "\n",
    "    # Show combined frame: webcam left, avatar right\n",
    "    combined_view = np.hstack((frame, avatar_canvas))\n",
    "    cv2.imshow(\"Real Face | Avatar Emotion\", cv2.resize(combined_view, (1280, 480)))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8390f871-f60f-451a-b01a-ab60eee278ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cartoon_avatar(canvas, emotion, cx, cy):\n",
    "    cv2.circle(canvas, (cx, cy), 50, (255, 255, 255), -1)  # head\n",
    "\n",
    "    # Eyes\n",
    "    cv2.circle(canvas, (cx - 15, cy - 15), 5, (0, 0, 0), -1)\n",
    "    cv2.circle(canvas, (cx + 15, cy - 15), 5, (0, 0, 0), -1)\n",
    "\n",
    "    # Mouth expressions\n",
    "    if emotion == \"happy\":\n",
    "        cv2.ellipse(canvas, (cx, cy + 10), (15, 10), 0, 0, 180, (0, 255, 0), 2)\n",
    "    elif emotion == \"sad\":\n",
    "        cv2.ellipse(canvas, (cx, cy + 20), (15, 10), 0, 180, 360, (255, 0, 0), 2)\n",
    "    elif emotion == \"angry\":\n",
    "        cv2.line(canvas, (cx - 15, cy + 15), (cx + 15, cy + 15), (0, 0, 255), 2)\n",
    "    else:\n",
    "        cv2.line(canvas, (cx - 10, cy + 15), (cx + 10, cy + 15), (200, 200, 200), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc03922-958a-45a8-91fb-c8486230f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def get_avatar_offset():\n",
    "    t = time.time()\n",
    "    offset = int(10 * math.sin(t * 3))  # Bouncing effect\n",
    "    return offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7032e5b-2e06-4209-8df5-0dbf047a8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "cx, cy = 320, 240  # Avatar center\n",
    "cy += get_avatar_offset()\n",
    "draw_cartoon_avatar(avatar_canvas, emotion, cx, cy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "254d26ae-a30d-44d8-ade7-285b828bf393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "last_spoken_emotion = None\n",
    "\n",
    "if emotion != last_spoken_emotion:\n",
    "    engine.say(f\"You look {emotion}\")\n",
    "    engine.runAndWait()\n",
    "    last_spoken_emotion = emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "985f7242-635f-4e36-8a02-01a0b5e9b7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-472 (<lambda>):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Deeksha\\AppData\\Local\\Temp\\ipykernel_31020\\2326136763.py\", line 74, in <lambda>\n",
      "  File \"C:\\Users\\Deeksha\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyttsx3\\engine.py\", line 180, in runAndWait\n",
      "    raise RuntimeError('run loop already started')\n",
      "RuntimeError: run loop already started\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Initialize TTS engine\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 160)\n",
    "\n",
    "# Mediapipe face mesh setup\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)\n",
    "drawing_utils = mp.solutions.drawing_utils\n",
    "\n",
    "# Facial landmark sets\n",
    "LIPS = mp_face_mesh.FACEMESH_LIPS\n",
    "LEFT_IRIS = [468, 469, 470, 471]\n",
    "RIGHT_IRIS = [473, 474, 475, 476]\n",
    "\n",
    "emotions = {\n",
    "    \"happy\": {\"emoji\": \"😊\", \"color\": (0, 255, 0), \"msg\": \"You look happy!\"},\n",
    "    \"sad\": {\"emoji\": \"😢\", \"color\": (255, 0, 0), \"msg\": \"You seem sad.\"},\n",
    "    \"angry\": {\"emoji\": \"😠\", \"color\": (0, 0, 255), \"msg\": \"You look angry!\"},\n",
    "    \"surprise\": {\"emoji\": \"😲\", \"color\": (0, 255, 255), \"msg\": \"You're surprised!\"},\n",
    "    \"neutral\": {\"emoji\": \"😐\", \"color\": (200, 200, 200), \"msg\": \"You look neutral.\"},\n",
    "    \"fear\": {\"emoji\": \"😨\", \"color\": (255, 140, 0), \"msg\": \"You look scared.\"},\n",
    "    \"disgust\": {\"emoji\": \"🤢\", \"color\": (138, 43, 226), \"msg\": \"You're feeling disgusted.\"}\n",
    "}\n",
    "\n",
    "prev_emotion = \"\"\n",
    "last_spoken_time = time.time()\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return np.linalg.norm(np.array([p1.x, p1.y]) - np.array([p2.x, p2.y]))\n",
    "\n",
    "def get_emotion(landmarks):\n",
    "    top_lip = landmarks[13]\n",
    "    bottom_lip = landmarks[14]\n",
    "    left_mouth = landmarks[61]\n",
    "    right_mouth = landmarks[291]\n",
    "    left_eye_top = landmarks[159]\n",
    "    left_eye_bottom = landmarks[145]\n",
    "    right_eye_top = landmarks[386]\n",
    "    right_eye_bottom = landmarks[374]\n",
    "    iris_left = landmarks[468]\n",
    "\n",
    "    face_width = distance(landmarks[234], landmarks[454])\n",
    "    mouth_open = distance(top_lip, bottom_lip) / face_width\n",
    "    mouth_stretch = distance(left_mouth, right_mouth) / face_width\n",
    "    eye_open = (distance(left_eye_top, left_eye_bottom) + distance(right_eye_top, right_eye_bottom)) / (2 * face_width)\n",
    "    eye_center_y = (left_eye_top.y + left_eye_bottom.y + right_eye_top.y + right_eye_bottom.y) / 4\n",
    "    sad_offset = iris_left.y - eye_center_y\n",
    "\n",
    "    if mouth_stretch > 0.40 and mouth_open < 0.06:\n",
    "        return \"happy\"\n",
    "    elif mouth_open >= 0.12:\n",
    "        return \"surprise\"\n",
    "    elif 0.06 < mouth_open < 0.12:\n",
    "        return \"fear\"\n",
    "    elif sad_offset > 0.01 and eye_open < 0.04:\n",
    "        return \"sad\"\n",
    "    elif mouth_open < 0.03 and eye_open < 0.08 and mouth_stretch < 0.38:\n",
    "        return \"disgust\"\n",
    "    elif eye_open > 0.096 and mouth_open < 0.06:\n",
    "        return \"angry\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def speak_emotion(msg):\n",
    "    global last_spoken_time\n",
    "    if time.time() - last_spoken_time > 2:\n",
    "        threading.Thread(target=lambda: engine.say(msg) or engine.runAndWait()).start()\n",
    "        last_spoken_time = time.time()\n",
    "\n",
    "# Start webcam (640x480)\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    avatar_canvas = np.zeros_like(frame)\n",
    "\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        landmark_list = face_landmarks.landmark\n",
    "\n",
    "        if frame_count % 5 == 0:  # Reduce emotion checks to every 5 frames\n",
    "            emotion = get_emotion(landmark_list)\n",
    "\n",
    "            if emotion != prev_emotion:\n",
    "                speak_emotion(emotions[emotion][\"msg\"])\n",
    "                prev_emotion = emotion\n",
    "        else:\n",
    "            emotion = prev_emotion\n",
    "\n",
    "        color = emotions[emotion][\"color\"]\n",
    "        emoji = emotions[emotion][\"emoji\"]\n",
    "\n",
    "        # Draw avatar only on right\n",
    "        drawing_utils.draw_landmarks(\n",
    "            avatar_canvas, face_landmarks,\n",
    "            mp_face_mesh.FACEMESH_TESSELATION, None,\n",
    "            drawing_utils.DrawingSpec(color=color, thickness=1, circle_radius=1)\n",
    "        )\n",
    "\n",
    "        for idx in LEFT_IRIS + RIGHT_IRIS:\n",
    "            pt = landmark_list[idx]\n",
    "            cx, cy = int(pt.x * frame.shape[1]), int(pt.y * frame.shape[0])\n",
    "            cv2.circle(avatar_canvas, (cx, cy), 2, (0, 255, 255), -1)\n",
    "\n",
    "        cx, cy = int(landmark_list[1].x * frame.shape[1]), int(landmark_list[1].y * frame.shape[0])\n",
    "        cv2.putText(avatar_canvas, f\"{emotion.upper()} {emoji}\", (cx - 40, cy - 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Combine webcam + avatar canvas\n",
    "    combined = np.hstack((frame, avatar_canvas))\n",
    "    cv2.imshow(\"Webcam | Avatar Emotion\", combined)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1ecc44-3a0f-4e13-8386-5142959e9e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mediapipe in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: pyttsx3 in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (2.98)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: absl-py in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (2.3.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: comtypes in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (from pyttsx3) (1.4.11)\n",
      "Requirement already satisfied: pypiwin32 in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: pywin32 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyttsx3) (305.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\deeksha\\appdata\\roaming\\python\\python312\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python pyttsx3 numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aff356b-37bc-4d67-9069-afbf1b3303c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Resize all avatars to 480x480\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m emotion_images:\n\u001b[1;32m---> 26\u001b[0m     emotion_images[key] \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(emotion_images[key], (\u001b[38;5;241m480\u001b[39m, \u001b[38;5;241m480\u001b[39m))\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Mediapipe FaceMesh\u001b[39;00m\n\u001b[0;32m     29\u001b[0m mp_face_mesh \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mface_mesh\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "# TTS engine\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 160)\n",
    "\n",
    "# Load cartoon avatar images\n",
    "emotion_images = {\n",
    "    \"happy\": cv2.imread(\"avatar/happy.png\"),\n",
    "    \"sad\": cv2.imread(\"avatar/sad.png\"),\n",
    "    \"angry\": cv2.imread(\"avatar/angry.png\"),\n",
    "    \"surprise\": cv2.imread(\"avatar/surprise.png\"),\n",
    "    \"fear\": cv2.imread(\"avatar/fear.png\"),\n",
    "    \"disgust\": cv2.imread(\"avatar/disgust.png\"),\n",
    "    \"neutral\": cv2.imread(\"avatar/base.png\")\n",
    "}\n",
    "\n",
    "# Resize all avatars to 480x480\n",
    "for key in emotion_images:\n",
    "    emotion_images[key] = cv2.resize(emotion_images[key], (480, 480))\n",
    "\n",
    "# Mediapipe FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)\n",
    "\n",
    "prev_emotion = \"\"\n",
    "last_spoken_time = time.time()\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return np.linalg.norm(np.array([p1.x, p1.y]) - np.array([p2.x, p2.y]))\n",
    "\n",
    "def get_emotion(landmarks):\n",
    "    top_lip = landmarks[13]\n",
    "    bottom_lip = landmarks[14]\n",
    "    left_mouth = landmarks[61]\n",
    "    right_mouth = landmarks[291]\n",
    "    left_eye_top = landmarks[159]\n",
    "    left_eye_bottom = landmarks[145]\n",
    "    right_eye_top = landmarks[386]\n",
    "    right_eye_bottom = landmarks[374]\n",
    "    iris_left = landmarks[468]\n",
    "\n",
    "    face_width = distance(landmarks[234], landmarks[454])\n",
    "    mouth_open = distance(top_lip, bottom_lip) / face_width\n",
    "    mouth_stretch = distance(left_mouth, right_mouth) / face_width\n",
    "    eye_open = (distance(left_eye_top, left_eye_bottom) + distance(right_eye_top, right_eye_bottom)) / (2 * face_width)\n",
    "    eye_center_y = (left_eye_top.y + left_eye_bottom.y + right_eye_top.y + right_eye_bottom.y) / 4\n",
    "    sad_offset = iris_left.y - eye_center_y\n",
    "\n",
    "    if mouth_stretch > 0.40 and mouth_open < 0.06:\n",
    "        return \"happy\"\n",
    "    elif mouth_open >= 0.12:\n",
    "        return \"surprise\"\n",
    "    elif 0.06 < mouth_open < 0.12:\n",
    "        return \"fear\"\n",
    "    elif sad_offset > 0.01 and eye_open < 0.04:\n",
    "        return \"sad\"\n",
    "    elif mouth_open < 0.03 and eye_open < 0.08 and mouth_stretch < 0.38:\n",
    "        return \"disgust\"\n",
    "    elif eye_open > 0.096 and mouth_open < 0.06:\n",
    "        return \"angry\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def speak_emotion(msg):\n",
    "    global last_spoken_time\n",
    "    if time.time() - last_spoken_time > 2:\n",
    "        threading.Thread(target=lambda: engine.say(msg) or engine.runAndWait()).start()\n",
    "        last_spoken_time = time.time()\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 480)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        landmark_list = face_landmarks.landmark\n",
    "\n",
    "        if frame_count % 5 == 0:\n",
    "            emotion = get_emotion(landmark_list)\n",
    "            if emotion != prev_emotion:\n",
    "                speak_emotion({\n",
    "                    \"happy\": \"You look happy!\",\n",
    "                    \"sad\": \"You seem sad.\",\n",
    "                    \"angry\": \"You look angry!\",\n",
    "                    \"surprise\": \"You're surprised!\",\n",
    "                    \"neutral\": \"You look neutral.\",\n",
    "                    \"fear\": \"You look scared.\",\n",
    "                    \"disgust\": \"You're feeling disgusted.\"\n",
    "                }[emotion])\n",
    "                prev_emotion = emotion\n",
    "        else:\n",
    "            emotion = prev_emotion\n",
    "    else:\n",
    "        emotion = \"neutral\"\n",
    "\n",
    "    # Get the cartoon avatar for detected emotion\n",
    "    avatar = emotion_images.get(emotion, emotion_images[\"neutral\"])\n",
    "    combined = np.hstack((frame, avatar))\n",
    "    cv2.imshow(\"Real Face | Avatar Emotion\", combined)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0c14cfb-6f79-4472-941f-e45577592d09",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'avatar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavatar\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'avatar'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"avatar\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2197d9d5-f266-4c20-bd5a-04cc7aeca220",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'avatar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavatar\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'avatar'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"avatar\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2456ab7-2b8e-4122-94c7-54ef3a86a833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-477 (speech_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Deeksha\\AppData\\Local\\Temp\\ipykernel_31020\\3817472991.py\", line 22, in speech_loop\n",
      "  File \"C:\\Users\\Deeksha\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyttsx3\\engine.py\", line 180, in runAndWait\n",
      "    raise RuntimeError('run loop already started')\n",
      "RuntimeError: run loop already started\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "\n",
    "# Initialize TTS engine\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 160)\n",
    "\n",
    "# Queue and threading for speech\n",
    "speech_queue = Queue()\n",
    "stop_signal = threading.Event()\n",
    "\n",
    "def speech_loop():\n",
    "    while not stop_signal.is_set():\n",
    "        if not speech_queue.empty():\n",
    "            msg = speech_queue.get()\n",
    "            engine.say(msg)\n",
    "            engine.runAndWait()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "def speak_emotion(msg):\n",
    "    if speech_queue.empty():\n",
    "        speech_queue.put(msg)\n",
    "\n",
    "# Mediapipe face mesh setup\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)\n",
    "drawing_utils = mp.solutions.drawing_utils\n",
    "\n",
    "# Facial landmark sets\n",
    "LIPS = mp_face_mesh.FACEMESH_LIPS\n",
    "LEFT_IRIS = [468, 469, 470, 471]\n",
    "RIGHT_IRIS = [473, 474, 475, 476]\n",
    "\n",
    "emotions = {\n",
    "    \"happy\": {\"emoji\": \"😊\", \"color\": (0, 255, 0), \"msg\": \"You look happy!\"},\n",
    "    \"sad\": {\"emoji\": \"😢\", \"color\": (255, 0, 0), \"msg\": \"You seem sad.\"},\n",
    "    \"angry\": {\"emoji\": \"😠\", \"color\": (0, 0, 255), \"msg\": \"You look angry!\"},\n",
    "    \"surprise\": {\"emoji\": \"😲\", \"color\": (0, 255, 255), \"msg\": \"You're surprised!\"},\n",
    "    \"neutral\": {\"emoji\": \"😐\", \"color\": (200, 200, 200), \"msg\": \"You look neutral.\"},\n",
    "    \"fear\": {\"emoji\": \"😨\", \"color\": (255, 140, 0), \"msg\": \"You look scared.\"},\n",
    "    \"disgust\": {\"emoji\": \"🤢\", \"color\": (138, 43, 226), \"msg\": \"You're feeling disgusted.\"}\n",
    "}\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return np.linalg.norm(np.array([p1.x, p1.y]) - np.array([p2.x, p2.y]))\n",
    "\n",
    "def get_emotion(landmarks):\n",
    "    top_lip = landmarks[13]\n",
    "    bottom_lip = landmarks[14]\n",
    "    left_mouth = landmarks[61]\n",
    "    right_mouth = landmarks[291]\n",
    "    left_eye_top = landmarks[159]\n",
    "    left_eye_bottom = landmarks[145]\n",
    "    right_eye_top = landmarks[386]\n",
    "    right_eye_bottom = landmarks[374]\n",
    "    iris_left = landmarks[468]\n",
    "\n",
    "    face_width = distance(landmarks[234], landmarks[454])\n",
    "    mouth_open = distance(top_lip, bottom_lip) / face_width\n",
    "    mouth_stretch = distance(left_mouth, right_mouth) / face_width\n",
    "    eye_open = (distance(left_eye_top, left_eye_bottom) + distance(right_eye_top, right_eye_bottom)) / (2 * face_width)\n",
    "    eye_center_y = (left_eye_top.y + left_eye_bottom.y + right_eye_top.y + right_eye_bottom.y) / 4\n",
    "    sad_offset = iris_left.y - eye_center_y\n",
    "\n",
    "    if mouth_stretch > 0.40 and mouth_open < 0.06:\n",
    "        return \"happy\"\n",
    "    elif mouth_open >= 0.12:\n",
    "        return \"surprise\"\n",
    "    elif 0.06 < mouth_open < 0.12:\n",
    "        return \"fear\"\n",
    "    elif sad_offset > 0.01 and eye_open < 0.04:\n",
    "        return \"sad\"\n",
    "    elif mouth_open < 0.03 and eye_open < 0.08 and mouth_stretch < 0.38:\n",
    "        return \"disgust\"\n",
    "    elif eye_open > 0.096 and mouth_open < 0.06:\n",
    "        return \"angry\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "prev_emotion = \"\"\n",
    "frame_count = 0\n",
    "\n",
    "# Start speaker thread\n",
    "speaker_thread = threading.Thread(target=speech_loop, daemon=True)\n",
    "speaker_thread.start()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    avatar_canvas = np.zeros_like(frame)\n",
    "\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        landmark_list = face_landmarks.landmark\n",
    "\n",
    "        if frame_count % 2 == 0:  # Faster detection\n",
    "            emotion = get_emotion(landmark_list)\n",
    "\n",
    "            if emotion != prev_emotion:\n",
    "                prev_emotion = emotion\n",
    "                speak_emotion(emotions[emotion][\"msg\"])\n",
    "        else:\n",
    "            emotion = prev_emotion\n",
    "\n",
    "        color = emotions[emotion][\"color\"]\n",
    "        emoji = emotions[emotion][\"emoji\"]\n",
    "\n",
    "        # Draw face mesh on avatar canvas\n",
    "        drawing_utils.draw_landmarks(\n",
    "            avatar_canvas, face_landmarks,\n",
    "            mp_face_mesh.FACEMESH_TESSELATION, None,\n",
    "            drawing_utils.DrawingSpec(color=color, thickness=1, circle_radius=1)\n",
    "        )\n",
    "\n",
    "        # Draw iris points\n",
    "        for idx in LEFT_IRIS + RIGHT_IRIS:\n",
    "            pt = landmark_list[idx]\n",
    "            cx, cy = int(pt.x * frame.shape[1]), int(pt.y * frame.shape[0])\n",
    "            cv2.circle(avatar_canvas, (cx, cy), 2, (0, 255, 255), -1)\n",
    "\n",
    "        # Show emotion text\n",
    "        cx, cy = int(landmark_list[1].x * frame.shape[1]), int(landmark_list[1].y * frame.shape[0])\n",
    "        cv2.putText(avatar_canvas, f\"{emotion.upper()} {emoji}\", (cx - 40, cy - 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Combine original + avatar canvas\n",
    "    combined = np.hstack((frame, avatar_canvas))\n",
    "    cv2.imshow(\"Webcam | Avatar Emotion\", combined)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to quit\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "stop_signal.set()\n",
    "speaker_thread.join()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f38f4-b1ab-43a3-9d20-b979a91d8c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
